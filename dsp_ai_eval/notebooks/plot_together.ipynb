{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import Dict, Any\n",
    "from umap import UMAP\n",
    "\n",
    "from nesta_ds_utils.viz.altair import saving as viz_save\n",
    "from dsp_ai_eval.getters.scite import get_scite_df_w_embeddings\n",
    "from dsp_ai_eval.getters.gpt import get_gpt_themes_embeddings, get_cluster_summaries_cleaned, get_topics\n",
    "\n",
    "from dsp_ai_eval import PROJECT_DIR, config, logging\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "model = SentenceTransformer(config[\"embedding_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_most_similar_abstracts(gpt_cluster_summaries: pd.DataFrame, \n",
    "                                 abstracts: pd.DataFrame, \n",
    "                                 n: int = 3) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Finds the n most similar abstracts for each cluster represented in the GPT cluster summaries.\n",
    "    \n",
    "    For each cluster, it takes the most representative document in that cluster (ie a GPT-generated sentence). Then it computes the cosine\n",
    "    similarity between that document's embedding and the embeddings of all the abstracts.\n",
    "\n",
    "    It returns a dictionary where each key is a topic name from the \n",
    "    GPT cluster summaries, and the value is a DataFrame containing the top n most similar abstracts, \n",
    "    along with their similarity scores and assigned topic.\n",
    "\n",
    "    Parameters:\n",
    "    - gpt_cluster_summaries (pd.DataFrame): A DataFrame with at least two columns: 'representative_docs' \n",
    "      which contains the representative documents for each cluster, and 'topic_name' which contains the name \n",
    "      of the topic associated with each cluster.\n",
    "    - abstracts (pd.DataFrame): A DataFrame containing the abstracts with their embeddings in a column named \n",
    "      'embeddings'. Each embedding should be stored in a format that can be converted to a pandas Series.\n",
    "    - n (int, optional): The number of similar abstracts to retrieve for each topic. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, pd.DataFrame]: A dictionary mapping each topic name to a DataFrame containing the top n similar \n",
    "      abstracts, with additional columns 'topic' and 'similarity' for the topic name and similarity score, respectively.\n",
    "    \"\"\"\n",
    "    most_similar_abstracts = {}\n",
    "\n",
    "    for _, row in gpt_cluster_summaries.iterrows():\n",
    "        doc = ast.literal_eval(row['representative_docs'])[0]\n",
    "        reference_embedding = model.encode(doc)\n",
    "        similarities = [cosine_similarity([reference_embedding], [embed])[0][0] for embed in abstracts['embeddings'].apply(pd.Series).values]\n",
    "        top_indices = np.argsort(similarities)[::-1][:n]\n",
    "        similar_abstracts = abstracts.iloc[top_indices]\n",
    "        similar_abstracts['topic'] = row['topic_name']\n",
    "        similar_abstracts['similarity'] = [similarities[i] for i in top_indices]\n",
    "        \n",
    "        most_similar_abstracts[row['topic_name']] = similar_abstracts\n",
    "        \n",
    "    return most_similar_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_long = get_gpt_themes_embeddings()\n",
    "abstracts = get_scite_df_w_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep the two datasets so that they have the same columns, then concatenate them\n",
    "abstracts = abstracts[['title_abstract', 'embeddings', 'total_cites']]\n",
    "abstracts = abstracts.rename(columns={'title_abstract': 'doc'})\n",
    "abstracts['gpt_model'] = 'research abstract'\n",
    "abstracts['temperature'] = 'NA'\n",
    "abstracts['source'] = 'abstract'\n",
    "abstracts['topic_name'] = '--'\n",
    "abstracts = abstracts[['doc', 'embeddings', 'total_cites', 'gpt_model', 'temperature', 'source', 'topic_name']]\n",
    "\n",
    "gpt_cluster_summaries = get_cluster_summaries_cleaned()\n",
    "topics = get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_long = answers_long[['answer_cleaned', 'embeddings', 'gpt_model', 'temperature']]\n",
    "answers_long[\"topic\"] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_long = pd.merge(answers_long, gpt_cluster_summaries, on='topic', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "answers_long['embeddings'] = answers_long['embeddings'].apply(ast.literal_eval)\n",
    "answers_long = answers_long.rename(columns={'answer_cleaned': 'doc'})\n",
    "answers_long['total_cites'] = 0\n",
    "answers_long['source'] = 'gpt'\n",
    "answers_long = answers_long[['doc', 'embeddings', 'total_cites', 'gpt_model', 'temperature', 'source', 'topic_name']]\n",
    "\n",
    "all_data = pd.concat([abstracts, answers_long], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many documents we have from the GPT responses vs how many research abstracts\n",
    "all_data['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're interested in finding out how many abstracts (if any) fall near clusters of GPT responses, which is why\n",
    "# we're only looking at GPT topic names for now\n",
    "all_data[\"topic_name\"].fillna(\"--\", inplace=True)\n",
    "all_data['topic_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = all_data['embeddings'].apply(pd.Series).values\n",
    "\n",
    "umap_2d = UMAP(random_state=42)\n",
    "embeddings_2d = umap_2d.fit_transform(embeddings)\n",
    "\n",
    "df_vis = pd.DataFrame(embeddings_2d, columns=[\"x\", \"y\"])\n",
    "\n",
    "df_vis = pd.concat([all_data, df_vis], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis['gpt_model'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opacity_condition = alt.condition(\n",
    "            alt.datum.source == \"abstract\", alt.value(0.5), alt.value(0.25)\n",
    "        )\n",
    "\n",
    "scatter_plot = alt.Chart(df_vis).mark_circle(size=100).encode(\n",
    "    x=alt.X('x:Q', axis=alt.Axis(ticks=False, labels=False, title=None, grid=False)),\n",
    "    y=alt.Y('y:Q', axis=alt.Axis(ticks=False, labels=False, title=None, grid=False)),\n",
    "    color=alt.Color('source'),\n",
    "    opacity=opacity_condition,\n",
    "    tooltip=['source', 'doc']\n",
    ").configure_legend(title=None, labelFontSize=20, titleFontSize=20).properties(width=800, height=600).interactive()\n",
    "\n",
    "# scatter_plot.save(PROJECT_DIR / f\"outputs/figures/gpt_abstracts_overlap.html\")\n",
    "# viz_save.save(scatter_plot, f\"gpt_abstracts_overlap\", PROJECT_DIR / \"outputs/figures\", save_png=True)\n",
    "\n",
    "scatter_plot.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because in the previous plot it can be hard to see if GPT summaries are obscuring research abstracts, in the next plot, we scale the size of the points by number of citations. I would hypothesise that abstracts that have been cited hundreds of times should be more influential and therefore more likely to be similar to GPT summaries. So perhaps where there are small, seemingly outlying clusters of GPT summaries, maybe there are actually a couple of highly influential research papers nearby?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a plot where point size is scaled by number of citations\n",
    "\n",
    "def map_citations_to_size(citations, quantile_values):\n",
    "    \n",
    "    a=int(quantile_values[0.25])\n",
    "    b=int(quantile_values[0.5])\n",
    "    c=int(quantile_values[0.75])\n",
    "    \n",
    "    if citations==0:\n",
    "        return 'NA'\n",
    "    elif 0 < citations < a:\n",
    "        return f'5-{a-1}'\n",
    "    elif a <= citations < b:\n",
    "        return f'{a}-{b-1}'\n",
    "    elif b <= citations < c:\n",
    "        return f'{b}-{c-1}'\n",
    "    else:\n",
    "        return f'{c}+'\n",
    "\n",
    "# Specify the desired quantiles as a list of probabilities\n",
    "quantiles = [0.25, 0.5, 0.75]\n",
    "\n",
    "# Use the quantile method to calculate the quantiles\n",
    "quantile_values = df_vis[df_vis['source']=='abstract']['total_cites'].quantile(quantiles)\n",
    "logging.info(f\"quantiles: {quantile_values}\")\n",
    "    \n",
    "df_vis['point_size'] = df_vis['total_cites'].apply(lambda x: map_citations_to_size(x, quantile_values))\n",
    "\n",
    "# df_vis['size'] = df_vis['total_cites'].apply(lambda x: 100 if x == 0 else x*2)\n",
    "\n",
    "a=int(quantile_values[0.25])\n",
    "b=int(quantile_values[0.5])\n",
    "c=int(quantile_values[0.75])\n",
    "\n",
    "print(df_vis['point_size'].value_counts())\n",
    "print(df_vis['point_size'].unique().tolist())\n",
    "\n",
    "# First, define the size encoding that will be common to both layers\n",
    "size_encode = alt.Size(\n",
    "    \"total_cites:Q\",\n",
    "    scale=alt.Scale(\n",
    "        range=[50, 2000]\n",
    "    ),\n",
    "    legend=alt.Legend(title=\"Number of citations\", titleFontSize=12, labelPadding=100, labelFontSize=12,\n",
    "                      symbolFillColor='blue'),\n",
    ")\n",
    "\n",
    "# Define the base chart with common encoding settings\n",
    "base_chart = alt.Chart(df_vis).mark_circle().transform_calculate(\n",
    "        jittered_x=\"datum.x + sqrt(-2*log(random()))*cos(2*PI*random())*0.4\",\n",
    "        jittered_y=\"datum.y + sqrt(-2*log(random()))*sin(2*PI*random())*0.4\"\n",
    "    ).encode(\n",
    "    x=alt.X('jittered_x:Q', axis=alt.Axis(ticks=False, labels=False, title=None,grid=False)),\n",
    "    y=alt.Y('jittered_y:Q', axis=alt.Axis(ticks=False, labels=False, title=None,grid=False)),\n",
    "    tooltip=['source', 'doc', 'point_size', 'total_cites']\n",
    ")\n",
    "\n",
    "# Create separate layers\n",
    "gpt_points = base_chart.transform_filter(alt.datum.source == 'gpt').mark_circle(color='#9B30FF', size=50).encode(\n",
    "    opacity=alt.value(0.1),\n",
    "    # size=size_encode\n",
    ")\n",
    "\n",
    "abstract_points = base_chart.transform_filter(alt.datum.source == 'abstract').mark_circle(color='#3CB371').encode(\n",
    "    opacity=alt.value(0.4),size=size_encode\n",
    ")\n",
    "\n",
    "# Layer the charts\n",
    "layered_chart = alt.layer(gpt_points, abstract_points).properties(width=900, height=600).interactive()\n",
    "\n",
    "layered_chart.save(PROJECT_DIR / f\"outputs/figures/gpt_abstracts_overlap.html\")\n",
    "viz_save.save(layered_chart, f\"gpt_abstracts_overlap\", PROJECT_DIR / \"outputs/figures\", save_png=True)\n",
    "\n",
    "# Display the chart\n",
    "layered_chart.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same plot, but this time with the GPT points coloured by topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_encode = alt.Size(\n",
    "    \"total_cites:Q\",\n",
    "    scale=alt.Scale(\n",
    "        range=[50, 2000]#[50, 100, 500, 1000, 2000]\n",
    "    ),\n",
    "    legend=alt.Legend(title=\"Number of citations\", titleFontSize=12, labelPadding=100, labelFontSize=12),\n",
    ")\n",
    "\n",
    "# Define the base chart with common encoding settings\n",
    "base_chart = alt.Chart(df_vis).transform_calculate(\n",
    "        # Adding jitter calculations to x and y fields directly\n",
    "        jittered_x=\"datum.x + sqrt(-2*log(random()))*cos(2*PI*random())*0.4\",\n",
    "        jittered_y=\"datum.y + sqrt(-2*log(random()))*sin(2*PI*random())*0.4\"\n",
    "    ).encode(\n",
    "    x=alt.X('jittered_x:Q', axis=alt.Axis(ticks=False, labels=False, title=None,grid=False)),\n",
    "    y=alt.Y('jittered_y:Q', axis=alt.Axis(ticks=False, labels=False, title=None,grid=False)),\n",
    "    size=size_encode,\n",
    "    # opacity=alt.value(0.25),\n",
    "    tooltip=['source', 'doc', 'point_size', 'total_cites', 'topic_name']\n",
    ")\n",
    "\n",
    "topic_color_encoding = alt.Color('topic_name:N', legend=alt.Legend(title=\"Topics\"))\n",
    "\n",
    "# Create separate layers\n",
    "gpt_points = base_chart.transform_filter(alt.datum.source == 'gpt').mark_circle().encode(\n",
    "    color=topic_color_encoding, #alt.value('#3CB371'),  # Change color as needed\n",
    "    # size=size_encode,\n",
    "    opacity=alt.value(0.1),\n",
    ")\n",
    "\n",
    "abstract_points = base_chart.transform_filter(alt.datum.source == 'abstract').mark_circle().encode(\n",
    "    color=alt.value('#9B30FF'),  # Change color as needed\n",
    "    # size=size_encode,\n",
    "    opacity=alt.value(0.3),\n",
    ")\n",
    "\n",
    "# Layer the charts\n",
    "layered_chart = alt.layer(gpt_points, abstract_points).properties(width=900, height=600).interactive()\n",
    "\n",
    "layered_chart.save(PROJECT_DIR / f\"outputs/figures/gpt_abstracts_overlap_topics.html\")\n",
    "viz_save.save(layered_chart, f\"gpt_abstracts_overlap_topics\", PROJECT_DIR / \"outputs/figures\", save_png=True)\n",
    "\n",
    "# Display the chart\n",
    "layered_chart.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate most similar papers\n",
    "\n",
    "For each cluster of GPT summaries, find the N most similar research abstracts.\n",
    "\n",
    "Using a metric such as cosine similarity is important because how the distances look visually may be misleading - this was pointed out by Max in a PR review. Cosine similarity gives a more reliable/holistic picture of how similar or different the two text vectors actually are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_cluster_summaries = get_cluster_summaries_cleaned()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar_abstracts = get_n_most_similar_abstracts(gpt_cluster_summaries, abstracts, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df = pd.concat(most_similar_abstracts.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df.to_csv(PROJECT_DIR / \"outputs/data/similar_abstracts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df['topic'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df[concatenated_df['topic']=='International Technology Transfer and its Impact on UK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df[concatenated_df['topic']=='Skill Development and Technology Diffusion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df[concatenated_df['topic']=='Regional Disparities in Technology Diffusion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsp_ai_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
