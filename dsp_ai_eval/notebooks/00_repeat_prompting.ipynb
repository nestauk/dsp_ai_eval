{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from dsp_ai_eval import PROJECT_DIR\n",
    "\n",
    "model = SentenceTransformer('all-miniLM-L6-v2')\n",
    "\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_data = pd.read_csv(PROJECT_DIR / 'inputs/data/repeated_prompts.csv')\n",
    "answers_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = answers_data['Answer'].tolist()\n",
    "indices = [i-1 for i in answers_data['Attempt'].tolist()]\n",
    "embeddings = model.encode(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "pd.DataFrame(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a network graph\n",
    "G = nx.Graph()\n",
    "for i, text in enumerate(texts):\n",
    "    G.add_node(i, label=text)\n",
    "\n",
    "# Adding edges based on similarity\n",
    "threshold = 0.75\n",
    "for i in range(len(texts)):\n",
    "    for j in range(i + 1, len(texts)):\n",
    "        if similarity_matrix[i][j] > threshold:\n",
    "            G.add_edge(i, j, weight=similarity_matrix[i][j])\n",
    "\n",
    "# Map index positions to colors\n",
    "colors = plt.cm.plasma(np.linspace(0, 1, len(indices)))\n",
    "color_map = [colors[i] for i in indices]\n",
    "\n",
    "# Plot the network graph without text labels\n",
    "pos = nx.spring_layout(G)  # positions for all nodes\n",
    "\n",
    "# nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_size=700, node_color=color_map)\n",
    "\n",
    "# edges\n",
    "weights = nx.get_edge_attributes(G, 'weight')\n",
    "nx.draw_networkx_edges(G, pos, width=[v*1.5 for v in weights.values()])\n",
    "\n",
    "# labels\n",
    "labels = {i: str(indices[i]) for i in range(len(texts))}\n",
    "nx.draw_networkx_labels(G, pos, labels=labels, font_size=12)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Removing stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Stemming (optional)\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    return \" \".join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the texts\n",
    "preprocessed_texts = [preprocess_text(text) for text in texts]\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(preprocessed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names to use as dataframe columns\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert to array and show the result\n",
    "tfidf_array = tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for better readability\n",
    "df = pd.DataFrame(tfidf_array, columns=feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find top n scoring words for each document\n",
    "def top_n_words_per_document(df, n=10):\n",
    "    top_words = {}\n",
    "    for index, row in df.iterrows():\n",
    "        sorted_row = row.sort_values(ascending=False)\n",
    "        top_n = sorted_row.head(n)\n",
    "        top_words[f\"Document {index+1}\"] = list(top_n.index)\n",
    "    return top_words\n",
    "\n",
    "# Get the top 10 scoring words for each document\n",
    "top_10_words = top_n_words_per_document(df, 10)\n",
    "\n",
    "# Display the results\n",
    "for doc, words in top_10_words.items():\n",
    "    print(f\"{doc}: {words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = PROJECT_DIR / 'outputs/data/initial_repeat_prompting/tfidf_matrix.csv'\n",
    "Path(output_dir).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pd.DataFrame(top_10_words).to_csv(output_dir, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
